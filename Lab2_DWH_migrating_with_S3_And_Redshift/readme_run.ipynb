{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Data Warehouse with Sparkify\n",
       "\n",
       "In this project, I will work on a demo version for a music streaming startup, which will migrate the user base and song database to cloud, with the 3 following files:\n",
       "\n",
       "- The main data resides in S3\n",
       "- The user activity log will locate on a directory JSON logs\n",
       "- The metadata of the song will be saved in log_data\n",
       "\n",
       "\n",
       "On the next session, I will work on an ETL pipeline to extract the data from S3 and stage them in Redshift and transform data into a set of dimensional tables for further analytics.\n",
       " \n",
       "![Header](./system_structure.png)\n",
       "\n",
       "Note that the following prerequisites is the need to complete this project:\n",
       "- Programming language: **Python** (boto3, configparser, psycopg2)\n",
       "- **AWS** Services: EC2, S3, IAM, Redshift\n",
       "\n",
       "# Project path structure\n",
       "The project structure follows this path:\n",
       "```plaintext\n",
       "ETL_DWH_AWS_Project\n",
       "├── etl_testing.ipynb\n",
       "├── create_tables.py\n",
       "├── dwh.cfg\n",
       "├── etl.py\n",
       "├── README.md\n",
       "└── sql_queries.py\n",
       "```\n",
       "# Data Structure\n",
       "## S3 Datasets structure\n",
       "In here I will mainly work with 2 datasets stored in S3, with the following directories in S3:\n",
       "- Song data: ```s3://udacity-dend/song_data```\n",
       "- Log data: ```s3://udacity-dend/log_data```\n",
       "- \n",
       "To properly read log data, we will need the following metadata file as well:\n",
       "- Log metadata: ```s3://udacity-dend/log_json_path.json```\n",
       "\n",
       "\n",
       "From these, I will convert them into a staging tables in S3, known as the staging_events and staging_songs with the following fields:\n",
       "\n",
       "- **staging_events**\n",
       "  - `artist`\n",
       "  - `auth`\n",
       "  - `firstName`\n",
       "  - `gender`\n",
       "  - `itemInSession`\n",
       "  - `lastName`\n",
       "  - `length`\n",
       "  - `level`\n",
       "  - `location`\n",
       "  - `method`\n",
       "  \n",
       "- **staging_songs**\n",
       "    - `num_songs`\n",
       "    - `artist_id`\n",
       "    - `artist_latitude`\n",
       "    - `artist_longtitude`\n",
       "    - `artist_location`\n",
       "    - `artist_name`\n",
       "    - `song_id`\n",
       "    - `title`\n",
       "    - `duration`\n",
       "    - `year`\n",
       "\n",
       "## ETL data structure\n",
       "After ```COPY``` data from the S3 to ETL, we will have 5 tables coming with their own properties to form a [star schema](https://www.databricks.com/glossary/star-schema)\n",
       "\n",
       "\n",
       "- **songplays** (fact table)\n",
       "  - `song_play_id`\n",
       "  - `start_time`\n",
       "  - `user_id`\n",
       "  - `level`\n",
       "  - `song_id`\n",
       "  - `artist_id`\n",
       "  - `session_id`\n",
       "  - `location`\n",
       "  - `user_agent`\n",
       "\n",
       "coming with 4 dimensional tables:\n",
       "\n",
       "- **users**\n",
       "  - `user_id`\n",
       "  - `first_name`\n",
       "  - `last_name`\n",
       "  - `gender`\n",
       "  - `level`\n",
       "\n",
       "- **songs**\n",
       "  - `song_id`\n",
       "  - `title`\n",
       "  - `artist_id`\n",
       "  - `year`\n",
       "  - `duration`\n",
       "\n",
       "- **artists**\n",
       "  - `artist_id`\n",
       "  - `name`\n",
       "  - `location`\n",
       "  - `latitude`\n",
       "  - `longitude`\n",
       "\n",
       "- **time**\n",
       "  - `start_time`\n",
       "  - `hour`\n",
       "  - `day`\n",
       "  - `week`\n",
       "  - `month`\n",
       "  - `year`\n",
       "  - `weekday`\n",
       " \n",
       "The structure of the data tables will look iike as followed: \n",
       "\n",
       "![Header](./DWH_AWS_ERD.png)\n",
       "\n",
       "# Configuration for execution\n",
       "\n",
       "## 1. Set up on AWS Services\n",
       "Creating resources on AWS using the AWS management console to support the Redshift data warehouse. \n",
       "#### Create an IAM Role\n",
       "* Create a `myRedshiftRole` IAM role with the `AmazonS3ReadOnlyAccess` permission policy attached\n",
       "#### Create Security Group for Redshift\n",
       "* Create a `redshift_security_group` security group that authorizes Redshift cluster access (with the default VPC)\n",
       "#### Create an IAM User for Redshift\n",
       "* Create an IAM user with below two permission policies attached, and create and save the `Access key` and `Security access key`\n",
       "    * `AmazonRedshiftFullAccess`\n",
       "    * `AmazonS3ReadOnlyAccess`\n",
       "#### Launch a Redshift Cluster\n",
       "* Create the `redshift-cluster-1` cluster that attaches the `myRedshiftRole` role and the `redshift_security_group` security group \n",
       "\n",
       "In fact, We can either config through AWS console or through coding. But in this casse, all the above configurations we will config through the AWS console.\n",
       "\n",
       "## 2. Set up on Jupyter files\n",
       "Following the execution order:\n",
       "- Execute the `create_tables.py` to create the tablesthat we have define.\n",
       "- Execute the `etl.py` to load the data to the S3 data tables, and then copying it to the Redshift ETL to form the star schema\n",
       "- After finishing the execution, we can test the query execution from the ETL through `etl_testing.ipynb` file\n",
       "\n",
       "# Some notification\n",
       "As the cost for AWS can be really costly, especially when we run the service like Redshift, so the choice of the cluster is important\n",
       "- For this data, my advice is to setup the node to 4, so that when executing the `etl.py` will only take about 20 minutes to finish the etl.\n",
       "- While setting up the data, the `copying step` I reccommend to scale down the data as we don't need the complex computation for a simple task for better speed execution with the command `COMPUPDATE OFF` and `STATUPDATE OFF`\n",
       "- After finishing the test, please delete the AWS Redshift cluter and VPC configuration to save the budget from unecessary money loss.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def read_markdown_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    display(Markdown(content))\n",
    "\n",
    "# Provide the path to your README.md file\n",
    "read_markdown_file('./README.md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
